# EnergyEfficientAI
Compare model compression methods (quantization, pruning, distillation) on a deep neural network.



## Project Overview
This project compares different **model compression methods**â€”**quantization**, **pruning**, and **knowledge distillation (KD)**â€”on deep neural networks using the **CIFAR-10 dataset**.  
It focuses on evaluating the trade-offs between:

- **Model size**  
- **Computational efficiency**  
- **Accuracy**  

---

## Installation

1. Make sure you have Python 3.8 or higher installed.  
2. Install the required Python libraries using the provided `requirements.txt`:


bash
pip install -r requirements.txt


---

## Project Structure

```
.
â”œâ”€â”€ Baseline_Model_Training.ipynb     # Train ResNet-18 and WideResNet-28-10 models
â”œâ”€â”€ Model_Pruning.ipynb               # Apply pruning to compress models
â”œâ”€â”€ Model_Quantization.ipynb          # Apply quantization
â”œâ”€â”€ Knowledge_Distillation.ipynb      # Apply knowledge distillation (KD)
â”œâ”€â”€ wide_resnet28_10.py               # WideResNet-28-10 architecture used in notebooks
â”œâ”€â”€ requirements.txt                  # Required Python libraries
â””â”€â”€ README.md                         # Project overview and instructions
```

---

## Usage Instructions

Run the notebooks in the following sequence for correct workflow:

1. **Baseline_Model_Training.ipynb**

   * Trains baseline models: ResNet-18 and WideResNet-28-10.
2. **Model_Pruning.ipynb**

   * Applies pruning to reduce model parameters.
3. **Model_Quantization.ipynb**

   * Performs quantization to reduce model size and improve efficiency.
4. **Knowledge_Distillation.ipynb**

   * Trains a student model using knowledge distillation.

> The file `wide_resnet28_10.py` contains the WideResNet-28-10 architecture used in both baseline training and KD notebooks.

---

## Flow Diagram

```text
Baseline_Model_Training.ipynb
            â†“
     Model_Pruning.ipynb
            â†“
   Model_Quantization.ipynb
            â†“
 Knowledge_Distillation.ipynb
```

---

## Author

**Dhruvin Sojitra**
ðŸ“§ Email: [dhruvin.sojitra@hof-university.de](mailto:dhruvin.sojitra@hof-university.de)

---

## References

* CIFAR-10 Dataset: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
